{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging using modified Viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "#Importing libraries\n",
    "import nltk, re, pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pprint, time\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')], [('Mr.', 'NOUN'), ('Vinken', 'NOUN'), ('is', 'VERB'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Elsevier', 'NOUN'), ('N.V.', 'NOUN'), (',', '.'), ('the', 'DET'), ('Dutch', 'NOUN'), ('publishing', 'VERB'), ('group', 'NOUN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "# reading the Treebank tagged sentences\n",
    "nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))\n",
    "print(nltk_data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3718\n",
      "196\n"
     ]
    }
   ],
   "source": [
    "# Splitting into train and test\n",
    "train_set, test_set = train_test_split(nltk_data,test_size=0.05, random_state =100)\n",
    "print(len(train_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95949"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting list of tagged words\n",
    "train_tagged_words = [tup for sent in train_set for tup in sent]\n",
    "len(train_tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'bright', 'sign', 'is', 'that', 'a', 'growing', 'number', 'of', 'women']\n",
      "12106\n"
     ]
    }
   ],
   "source": [
    "# tokens \n",
    "tokens = [pair[0] for pair in train_tagged_words]\n",
    "print(tokens[:10])\n",
    "# vocabulary\n",
    "V = list(set(tokens))\n",
    "print(len(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "['PRT', 'PRON', 'CONJ', '.', 'ADJ', 'NUM', 'NOUN', 'VERB', 'DET', 'ADV', 'X', 'ADP']\n"
     ]
    }
   ],
   "source": [
    "# number of tags\n",
    "T = list(set([pair[1] for pair in train_tagged_words]))\n",
    "print(len(T))\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the vanilla Viterbi based POS tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emission Probabilities and Transition Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute word given tag: Emission Probability\n",
    "def word_given_tag(word, tag):\n",
    "    tag_list = [pair for pair in train_tagged_words if pair[1]==tag]\n",
    "    count_tag = len(tag_list)\n",
    "    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n",
    "    count_w_given_tag = len(w_given_tag_list)\n",
    "    \n",
    "    return (count_w_given_tag/count_tag)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute tag given tag: tag2(t2) given tag1 (t1), i.e. Transition Probability\n",
    "\n",
    "def t2_given_t1(t2, t1, train_bag = train_tagged_words):\n",
    "    tags = [pair[1] for pair in train_bag]\n",
    "    count_t1 = len([t for t in tags if t==t1])\n",
    "    count_t2_t1 = 0\n",
    "    for index in range(len(tags)-1):\n",
    "        if tags[index]==t1 and tags[index+1] == t2:\n",
    "            count_t2_t1 += 1\n",
    "    return (count_t2_t1/count_t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tags X Tags  Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRT</th>\n",
       "      <th>PRON</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>.</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>NUM</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>VERB</th>\n",
       "      <th>DET</th>\n",
       "      <th>ADV</th>\n",
       "      <th>X</th>\n",
       "      <th>ADP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PRT</th>\n",
       "      <td>0.001969</td>\n",
       "      <td>0.017717</td>\n",
       "      <td>0.002297</td>\n",
       "      <td>0.043635</td>\n",
       "      <td>0.083661</td>\n",
       "      <td>0.056102</td>\n",
       "      <td>0.245735</td>\n",
       "      <td>0.405184</td>\n",
       "      <td>0.101050</td>\n",
       "      <td>0.010171</td>\n",
       "      <td>0.013123</td>\n",
       "      <td>0.019357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>0.012261</td>\n",
       "      <td>0.007663</td>\n",
       "      <td>0.004981</td>\n",
       "      <td>0.040613</td>\n",
       "      <td>0.072031</td>\n",
       "      <td>0.007280</td>\n",
       "      <td>0.211494</td>\n",
       "      <td>0.484291</td>\n",
       "      <td>0.009195</td>\n",
       "      <td>0.034100</td>\n",
       "      <td>0.092720</td>\n",
       "      <td>0.023372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONJ</th>\n",
       "      <td>0.003709</td>\n",
       "      <td>0.058414</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.035698</td>\n",
       "      <td>0.118683</td>\n",
       "      <td>0.042188</td>\n",
       "      <td>0.350487</td>\n",
       "      <td>0.155308</td>\n",
       "      <td>0.118683</td>\n",
       "      <td>0.053778</td>\n",
       "      <td>0.008809</td>\n",
       "      <td>0.053778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.002511</td>\n",
       "      <td>0.065208</td>\n",
       "      <td>0.058032</td>\n",
       "      <td>0.092923</td>\n",
       "      <td>0.043681</td>\n",
       "      <td>0.081353</td>\n",
       "      <td>0.222531</td>\n",
       "      <td>0.088708</td>\n",
       "      <td>0.173558</td>\n",
       "      <td>0.052292</td>\n",
       "      <td>0.026908</td>\n",
       "      <td>0.092206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>0.010156</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.016052</td>\n",
       "      <td>0.063882</td>\n",
       "      <td>0.067158</td>\n",
       "      <td>0.020803</td>\n",
       "      <td>0.700901</td>\n",
       "      <td>0.011794</td>\n",
       "      <td>0.004914</td>\n",
       "      <td>0.004914</td>\n",
       "      <td>0.020311</td>\n",
       "      <td>0.078624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0.026144</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>0.013072</td>\n",
       "      <td>0.118835</td>\n",
       "      <td>0.033571</td>\n",
       "      <td>0.184195</td>\n",
       "      <td>0.352347</td>\n",
       "      <td>0.016934</td>\n",
       "      <td>0.003862</td>\n",
       "      <td>0.002674</td>\n",
       "      <td>0.211824</td>\n",
       "      <td>0.035056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>0.043357</td>\n",
       "      <td>0.004721</td>\n",
       "      <td>0.042921</td>\n",
       "      <td>0.239951</td>\n",
       "      <td>0.012165</td>\n",
       "      <td>0.009550</td>\n",
       "      <td>0.264280</td>\n",
       "      <td>0.146955</td>\n",
       "      <td>0.013363</td>\n",
       "      <td>0.016813</td>\n",
       "      <td>0.028868</td>\n",
       "      <td>0.177058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>0.031427</td>\n",
       "      <td>0.035916</td>\n",
       "      <td>0.005186</td>\n",
       "      <td>0.034291</td>\n",
       "      <td>0.065640</td>\n",
       "      <td>0.022448</td>\n",
       "      <td>0.111386</td>\n",
       "      <td>0.168744</td>\n",
       "      <td>0.133292</td>\n",
       "      <td>0.082050</td>\n",
       "      <td>0.218438</td>\n",
       "      <td>0.091184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.003727</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.017913</td>\n",
       "      <td>0.204977</td>\n",
       "      <td>0.021640</td>\n",
       "      <td>0.637293</td>\n",
       "      <td>0.040394</td>\n",
       "      <td>0.005771</td>\n",
       "      <td>0.012623</td>\n",
       "      <td>0.045323</td>\n",
       "      <td>0.009618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>0.014314</td>\n",
       "      <td>0.015646</td>\n",
       "      <td>0.006991</td>\n",
       "      <td>0.135153</td>\n",
       "      <td>0.130160</td>\n",
       "      <td>0.031624</td>\n",
       "      <td>0.031624</td>\n",
       "      <td>0.344541</td>\n",
       "      <td>0.069907</td>\n",
       "      <td>0.077230</td>\n",
       "      <td>0.023302</td>\n",
       "      <td>0.119507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0.184891</td>\n",
       "      <td>0.055705</td>\n",
       "      <td>0.010316</td>\n",
       "      <td>0.162831</td>\n",
       "      <td>0.016505</td>\n",
       "      <td>0.002857</td>\n",
       "      <td>0.062371</td>\n",
       "      <td>0.204571</td>\n",
       "      <td>0.055229</td>\n",
       "      <td>0.025393</td>\n",
       "      <td>0.074433</td>\n",
       "      <td>0.144898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0.001484</td>\n",
       "      <td>0.069119</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>0.039754</td>\n",
       "      <td>0.107389</td>\n",
       "      <td>0.061910</td>\n",
       "      <td>0.321213</td>\n",
       "      <td>0.008481</td>\n",
       "      <td>0.323969</td>\n",
       "      <td>0.013357</td>\n",
       "      <td>0.034984</td>\n",
       "      <td>0.017492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           PRT      PRON      CONJ         .       ADJ       NUM      NOUN  \\\n",
       "PRT   0.001969  0.017717  0.002297  0.043635  0.083661  0.056102  0.245735   \n",
       "PRON  0.012261  0.007663  0.004981  0.040613  0.072031  0.007280  0.211494   \n",
       "CONJ  0.003709  0.058414  0.000464  0.035698  0.118683  0.042188  0.350487   \n",
       ".     0.002511  0.065208  0.058032  0.092923  0.043681  0.081353  0.222531   \n",
       "ADJ   0.010156  0.000491  0.016052  0.063882  0.067158  0.020803  0.700901   \n",
       "NUM   0.026144  0.001485  0.013072  0.118835  0.033571  0.184195  0.352347   \n",
       "NOUN  0.043357  0.004721  0.042921  0.239951  0.012165  0.009550  0.264280   \n",
       "VERB  0.031427  0.035916  0.005186  0.034291  0.065640  0.022448  0.111386   \n",
       "DET   0.000240  0.003727  0.000481  0.017913  0.204977  0.021640  0.637293   \n",
       "ADV   0.014314  0.015646  0.006991  0.135153  0.130160  0.031624  0.031624   \n",
       "X     0.184891  0.055705  0.010316  0.162831  0.016505  0.002857  0.062371   \n",
       "ADP   0.001484  0.069119  0.000848  0.039754  0.107389  0.061910  0.321213   \n",
       "\n",
       "          VERB       DET       ADV         X       ADP  \n",
       "PRT   0.405184  0.101050  0.010171  0.013123  0.019357  \n",
       "PRON  0.484291  0.009195  0.034100  0.092720  0.023372  \n",
       "CONJ  0.155308  0.118683  0.053778  0.008809  0.053778  \n",
       ".     0.088708  0.173558  0.052292  0.026908  0.092206  \n",
       "ADJ   0.011794  0.004914  0.004914  0.020311  0.078624  \n",
       "NUM   0.016934  0.003862  0.002674  0.211824  0.035056  \n",
       "NOUN  0.146955  0.013363  0.016813  0.028868  0.177058  \n",
       "VERB  0.168744  0.133292  0.082050  0.218438  0.091184  \n",
       "DET   0.040394  0.005771  0.012623  0.045323  0.009618  \n",
       "ADV   0.344541  0.069907  0.077230  0.023302  0.119507  \n",
       "X     0.204571  0.055229  0.025393  0.074433  0.144898  \n",
       "ADP   0.008481  0.323969  0.013357  0.034984  0.017492  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating t x t transition matrix of tags. Each column is t2, each row is t1. Thus M(i, j) represents P(tj given ti)\n",
    "tags_matrix = np.zeros((len(T), len(T)), dtype='float32')\n",
    "for i, t1 in enumerate(list(T)):\n",
    "    for j, t2 in enumerate(list(T)): \n",
    "        tags_matrix[i, j] = t2_given_t1(t2, t1)\n",
    "        \n",
    "# convert the matrix to a df for better readability\n",
    "tags_df = pd.DataFrame(tags_matrix, columns = list(T), index=list(T))\n",
    "tags_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create plain version of Viterbi Algorithm which can not correctly identify the unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi Heuristic\n",
    "def Vanilla_viterbi(words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_tagged_words]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            #emission_p = word_given_tag(words[key], tag)\n",
    "            #word_tag_df\n",
    "            emission_p = word_given_tag(words[key], tag)\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        # getting state for which probability is maximum\n",
    "        state_max = T[p.index(pmax)] \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# list of tagged words\n",
    "test_run_base = [tup for sent in test_set for tup in sent]\n",
    "\n",
    "# list of untagged words\n",
    "test_tagged_words = [tup[0] for sent in test_set for tup in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy of Vanila Viterbi alogorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Investors', 'NOUN'), ('took', 'VERB'), ('advantage', 'NOUN'), ('of', 'ADP'), ('Tuesday', 'NOUN'), (\"'s\", 'PRT'), ('stock', 'NOUN'), ('rally', 'NOUN'), ('*-1', 'X'), ('to', 'PRT')]\n",
      "CPU times: user 8min 20s, sys: 2.49 s, total: 8min 22s\n",
      "Wall time: 8min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tagged_seq = Vanilla_viterbi(test_tagged_words)\n",
    "print(tagged_seq[:10])\n",
    "\n",
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \n",
    "accuracy = len(check)/len(tagged_seq)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Solve the problem of unknown words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the rule based function which can return the tag based on the pattern defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify patterns for tagging. example from the NLTK book\n",
    "\n",
    "\n",
    "patterns = [\n",
    "    (r'.*ing$', 'VERB'),              \n",
    "    (r'.*ed$', 'VERB'),               \n",
    "    (r'.*es$', 'VERB'),               \n",
    "    (r'.*ly$', 'ADV'),              \n",
    "    (r'.*ble$', 'ADJ'),                \n",
    "    (r'^-?[0-9]+(.[0-9]+)?$', 'NUM'), \n",
    "    (r'(?<=[0-9])(?:st|nd|rd|th)$', 'NUM'), #ordinal Numbers\n",
    "    (r'.*', 'NOUN')                    \n",
    "]\n",
    "\n",
    "regexp_tagger = nltk.RegexpTagger(patterns)\n",
    "\n",
    "def rulebased_tagging(test_words):\n",
    "    return regexp_tagger.tag([test_words])[0][1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Updated Viterbi Algorithms for different techniques\n",
    "\n",
    "### Probabilistic with Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated Viterbi Heuristic\n",
    "\n",
    "def Viterbi_probabilistic(words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_tagged_words]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = []\n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            #emission_p = word_given_tag(words[key], tag)\n",
    "            #word_tag_df\n",
    "            if(words[key] in V):\n",
    "                emission_p = word_given_tag(words[key], tag)\n",
    "                state_probability = emission_p * transition_p\n",
    "            else:\n",
    "                state_probability = transition_p\n",
    "                                   \n",
    "            p.append(state_probability)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        # getting state for which probability is maximum\n",
    "        state_max = T[p.index(pmax)] \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic with rule based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Viterbi_rulebased(words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_tagged_words]))\n",
    "    for key, word in enumerate(words):\n",
    "    #initialise list of probability column for a given observation\n",
    "        p = []\n",
    "        state_probability = 0\n",
    "        rule_tag = \"\"\n",
    "        if words[key] in V:\n",
    "            for tag in T:\n",
    "                if key == 0:\n",
    "                    transition_p = tags_df.loc['.', tag]\n",
    "                else:\n",
    "                    transition_p = tags_df.loc[state[-1], tag]\n",
    "                # compute emission and state probabilities\n",
    "                emission_p = word_given_tag(words[key], tag)\n",
    "                state_probability = emission_p * transition_p\n",
    "                p.append(state_probability)\n",
    "            pmax = max(p)\n",
    "            state_max = T[p.index(pmax)]\n",
    "            state.append(state_max)\n",
    "            \n",
    "        else:\n",
    "            rule_tag=rulebased_tagging(words[key])\n",
    "            state.append(rule_tag)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the tagging accuracies of the modifications with the Vanilla Viterbi algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modified Viterbi by using  Transition Probability to get tags for unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9331499894224666\n",
      "CPU times: user 8min 13s, sys: 2.31 s, total: 8min 16s\n",
      "Wall time: 8min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tagged_seq1 = Viterbi_probabilistic(test_tagged_words)\n",
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq1, test_run_base) if i == j] \n",
    "accuracy = len(check)/len(tagged_seq)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modified Viterbi by using rule based tagger method to evaluate tags for unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9481700867357732\n",
      "CPU times: user 8min 11s, sys: 3.52 s, total: 8min 14s\n",
      "Wall time: 8min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tagged_seq2 = Viterbi_rulebased(test_tagged_words)\n",
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq2, test_run_base) if i == j] \n",
    "accuracy = len(check)/len(tagged_seq)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NLTK Post Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Android', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('mobile', 'ADJ'),\n",
       " ('operating', 'NOUN')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "# nltk.download('brown')\n",
    "brown_news_tagged = brown.tagged_words(categories='news', tagset='universal')\n",
    "\n",
    "import requests\n",
    "response = requests.get('https://cdn.upgrad.com/UpGrad/temp/9dca5f3b-53c3-47e1-86d5-5ec5dafad6f0/Test_sentences.txt')\n",
    "data = response.text\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sentense = sent_tokenize(data)\n",
    "word_sample= word_tokenize(data)\n",
    "\n",
    "\n",
    "# calculating tags using nltk.pos_tags, tagset universal\n",
    "\n",
    "tagged_data=nltk.pos_tag(word_sample,tagset='universal')\n",
    "# tagged_data\n",
    "sample_sent =list([pair[0] for pair in tagged_data])\n",
    "sample_sent[:5]\n",
    "tagged_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Android', 'PRT'), ('is', 'VERB'), ('a', 'DET'), ('mobile', 'ADJ'), ('operating', 'NOUN')]\n",
      "0.7624309392265194\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "tagged_seq3 = Vanilla_viterbi(sample_sent)\n",
    "print(tagged_seq3[:5])\n",
    "\n",
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq3, tagged_data) if i == j] \n",
    "accuracy = len(check)/len(tagged_seq3)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_seq4 = Viterbi_probabilistic(sample_sent)\n",
    "print(tagged_seq4[:5])\n",
    "\n",
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq4, tagged_data) if i == j] \n",
    "accuracy = len(check)/len(tagged_seq4)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tagged_seq5 = Viterbi_rulebased(sample_sent)\n",
    "print(tagged_seq5[:5])\n",
    "\n",
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq5, tagged_data) if i == j] \n",
    "accuracy = len(check)/len(tagged_seq5)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
